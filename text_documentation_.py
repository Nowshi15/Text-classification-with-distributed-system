# -*- coding: utf-8 -*-
"""Text documentation .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j-1yYQmFeI0qHbe7zg8gIZiFhXUkwSnt
"""

!pip install pyspark==3.1.2 spark-nlp==3.3.4

import sparknlp
from pyspark.sql import SparkSession
from sparknlp.base import DocumentAssembler
from sparknlp.annotator import Tokenizer, Normalizer, LemmatizerModel
from pyspark.ml import Pipeline
from pyspark.sql.functions import col, explode, array_join, collect_list

spark = sparknlp.start()

document_assembler = DocumentAssembler() \
    .setInputCol("review") \
    .setOutputCol("document")

tokenizer = Tokenizer() \
    .setInputCols(["document"]) \
    .setOutputCol("token")

normalizer = Normalizer() \
    .setInputCols(["token"]) \
    .setOutputCol("normalized") \
    .setLowercase(True) \
    .setCleanupPatterns([r"[^\w\d\s]"])  # Remove punctuation and non-alphanumeric characters

lemmatizer = LemmatizerModel.pretrained("lemma_antbnc", "en") \
    .setInputCols(["normalized"]) \
    .setOutputCol("lemma")

pipeline = Pipeline(stages=[
    document_assembler,
    tokenizer,
    normalizer,
    lemmatizer
])

data = spark.read.csv("IMDB Dataset.csv", header=True, inferSchema=True)

result = pipeline.fit(data).transform(data)
processed_data = result.withColumn("tokens", explode(col("lemma.result")))

updated_data = processed_data.groupBy("review").agg(array_join(collect_list("tokens"), " ").alias("processed_text"))

# Show the updated dataset
updated_data.show(truncate=False)

# Save the updated dataset to a CSV file
updated_data.write.format("csv").option("header", True).save("processed_imdb_reviews.csv")

from google.colab import files
files.download('/content/processed_imdb_reviews.csv')